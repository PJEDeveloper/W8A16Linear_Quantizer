{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab154f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from mistral_inference.transformer import Transformer\n",
    "from mistral_inference.generate import generate\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "from mistral_common.protocol.instruct.messages import UserMessage\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74a41db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the w8_a16_forward function\n",
    "def w8_a16_forward(weight, input, scales, bias=None):\n",
    "    casted_weights = weight.to(input.dtype)  # Cast weights to the input's dtype\n",
    "    output = F.linear(input, casted_weights) * scales  # Apply scales after the linear transformation\n",
    "\n",
    "    if bias is not None:\n",
    "        output = output + bias  # Add bias if provided\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60483ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define W8A16LinearLayer class\n",
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "\n",
    "        # Register the buffers for int8 weights and scales\n",
    "        self.register_buffer(\n",
    "            \"int8_weights\", \n",
    "            torch.randint(-128, 127, (out_features, in_features), dtype=torch.int8)\n",
    "        )\n",
    "        self.register_buffer(\"scales\", torch.randn((out_features), dtype=dtype))\n",
    "\n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\", torch.randn((1, out_features), dtype=dtype))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    # Quantize method for converting weights to int8\n",
    "    def quantize(self, weights):\n",
    "        w_fp32 = weights.clone().to(torch.float32)\n",
    "        scales = w_fp32.abs().max(dim=-1).values / 127\n",
    "        scales = scales.to(weights.dtype)\n",
    "        int8_weights = torch.round(weights / scales.unsqueeze(1)).to(torch.int8)\n",
    "\n",
    "        self.int8_weights = int8_weights\n",
    "        self.scales = scales\n",
    "\n",
    "    # Forward method for the layer\n",
    "    def forward(self, input):\n",
    "        return w8_a16_forward(self.int8_weights, input, self.scales, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace linear layers with the quantized version\n",
    "def replace_linear_with_target_and_quantize(module, target_class, module_name_to_exclude):\n",
    "    for name, child in module.named_children():\n",
    "        # Check if the layer is a Linear layer and not excluded\n",
    "        if isinstance(child, nn.Linear) and not any([x == name for x in module_name_to_exclude]):\n",
    "            old_bias = child.bias\n",
    "            old_weight = child.weight\n",
    "\n",
    "            # Create the new quantized module\n",
    "            new_module = target_class(child.in_features, child.out_features, old_bias is not None, child.weight.dtype)\n",
    "            setattr(module, name, new_module)\n",
    "\n",
    "            # Quantize the old weight and replace it in the new module\n",
    "            getattr(module, name).quantize(old_weight)\n",
    "            \n",
    "            # Retain the old bias\n",
    "            if old_bias is not None:\n",
    "                getattr(module, name).bias = old_bias\n",
    "        else:\n",
    "            # Recursively apply the quantization replacement to nested modules\n",
    "            replace_linear_with_target_and_quantize(child, target_class, module_name_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cd5636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "mistral_models_path = Path.home().joinpath('mistral_models', '7B-Instruct-v0.3')\n",
    "mistral_models_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "snapshot_download(repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", \n",
    "                  allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], \n",
    "                  local_dir=mistral_models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a788624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model using the mistral_inference library\n",
    "tokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tokenizer.model.v3\")\n",
    "model = Transformer.from_folder(mistral_models_path).cuda()\n",
    "print(\"Model before:\\n\\n\", mistral_models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c28b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print the weights of all Linear layers before quantization\n",
    "def print_linear_weights_before_quantization(model):\n",
    "    print(\"Weights before quantization:\\n\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            print(f\"Layer: {name}\")\n",
    "            print(f\"Weights: {param.data}\")  # .data gives the raw tensor of weights\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "# Call the function to print weights before quantization\n",
    "print_linear_weights_before_quantization(model)\n",
    "\n",
    "# Create an example chat completion request\n",
    "completion_request = ChatCompletionRequest(\n",
    "    messages=[UserMessage(content=\"Explain Machine Learning to me in a nutshell.\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eb12b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the chat completion request\n",
    "tokens = tokenizer.encode_chat_completion(completion_request).tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e73111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model before quantization and measure the inference time\n",
    "start_time = time.time()\n",
    "out_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n",
    "result_before = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n",
    "end_time = time.time()\n",
    "\n",
    "time_before_quantization = end_time - start_time\n",
    "print(f\"Before Quantization: {result_before}\")\n",
    "print(f\"Inference Time Before Quantization: {time_before_quantization:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583e03da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply quantization\n",
    "replace_linear_with_target_and_quantize(model, W8A16LinearLayer, [\"lm_head\"])\n",
    "\n",
    "print(\"Model before:\\n\\n\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30435ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model after quantization and measure the inference time\n",
    "start_time = time.time()\n",
    "out_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n",
    "result_after = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n",
    "end_time = time.time()\n",
    "\n",
    "time_after_quantization = end_time - start_time\n",
    "print(f\"After Quantization: {result_after}\")\n",
    "print(f\"Inference Time After Quantization: {time_after_quantization:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01d16ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "time_difference = time_before_quantization - time_after_quantization\n",
    "print(f\"Time saved with quantization: {time_difference:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce8495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print quantized weights and scales after quantization\n",
    "def print_quantized_weights(model):\n",
    "    print(\"Weights after quantization:\\n\")\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, W8A16LinearLayer):\n",
    "            print(f\"Layer: {name}\")\n",
    "            print(f\"Quantized Weights (int8):\\n{module.int8_weights}\")\n",
    "            print(f\"Scales:\\n{module.scales}\")\n",
    "            if module.bias is not None:\n",
    "                print(f\"Bias (still in FP32):\\n{module.bias}\")\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ba46d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to print quantized weights and scales\n",
    "print_quantized_weights(model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
